{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Variational_Autoencoder.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"fVhKv6BFUNMN","colab_type":"code","colab":{}},"source":["########################################\n","##...... Variational AutoEncoder .....##\n","########################################\n","\n","\n","\n","# Import Libraries\n","from __future__ import print_function, division\n","from builtins import range\n","\n","import numpy as np \n","import tensorflow as tf \n","import matplotlib.pyplot as plt\n","\n","from keras.datasets import mnist \n","\n","st  = None\n","try:\n","    # If this module still exists, use it\n","    st = tf.contrib.bayesflow.stochastic_tensor\n","except:\n","    pass\n","\n","\n","# Save distributions in a shortened form\n","Normal = tf.contrib.distributions.Normal\n","Bernoulli = tf.contrib.distributions.Bernoulli\n","\n","\n","\n","# Dense Layer object\n","class DenseLayer(object):\n","    # Define layer parameters \n","    def __init__(self, L1, L2, A = tf.nn.relu):\n","         self.W = tf.Variable(tf.random_normal(shape = (L1, L2)) * 2 / np.sqrt(L1))\n","         self.b = tf.Variable(np.zeros(L2).astype(np.float32))\n","         self.A = A\n","\n","    # Compute forward transfer through one layer \n","    def forward(self, X):\n","        return self.A(tf.matmul(X, self.W) + self.b)\n","\n","\n","\n","# Class for Variational Autoencoder\n","class Variational_Autoencoder:\n","\n","    \"\"\"\n","    Class to execute a Variational Autoencoder.\n","\n","    Class definition requires two inputs:\n","    D -> The number of features in the input\n","    hidden_layer_sizes -> The number of neurons or sizes of all hidden layers in the encoder step\n","\n","    Once defined, Mnist or other dataset can be used to execute the fit function to initiate the training.\n","\n","    Please read individual function definitions for more information.\n","    \"\"\"\n","    \n","    def __init__(self, D, hidden_layer_sizes):\n","\n","        \"\"\"\n","        Defines the necessary variables, layers, cost functions and other initialisers. \n","        \"\"\"\n","        self.X = tf.placeholder(tf.float32, shape = (None, D))\n","\n","\n","        #... ENCODER ...#\n","        # Define the encoder layers and compute the layer values to find the latent variable distribution.\n","        # The means and standard deviations are obtained after the last layer.\n","\n","        self.encoder_layers = []\n","        L_in = D\n","\n","        for L_out in hidden_layer_sizes[:-1]:\n","            h = DenseLayer(L_in, L_out)\n","            self.encoder_layers.append(h)\n","            L_in = L_out\n","\n","        L = hidden_layer_sizes[-1]\n","\n","        h = DenseLayer(L_in, 2 * L, A = lambda x: x)\n","        self.encoder_layers.append(h)\n","\n","        current_layer = self.X\n","        for layer in self.encoder_layers:\n","            current_layer = layer.forward(current_layer)\n","\n","        self.means = current_layer[:, :L]\n","        self.stddev = tf.nn.softplus(current_layer[:, L:]) + 1e-6\n","        \n","        # Find the distributions\n","        if st is None:\n","            standard_normal = Normal(\n","                loc = np.zeros(L, dtype=np.float32),\n","                scale = np.ones(L, dtype=np.float32)\n","            )\n","            e = standard_normal.sample(tf.shape(self.means)[0])\n","            self.Z = e * self.stddev + self.means\n","        else:\n","            with st.value_type(st.SampleValue()):\n","                self.Z = st.StochasticTensor(Normal(loc=self.means, scale=self.stddev))\n","        \n","\n","        #... DECODER ...#\n","        # Define the decoder layers and compute the layer values to find the posterior and prior distributions\n","        \n","        self.decoder_layers = []\n","        L_in = L\n","\n","        for L_out in reversed(hidden_layer_sizes[ :-1]):\n","            h = DenseLayer(L_in, L_out)\n","            self.decoder_layers.append(h)\n","            L_in = L_out\n","\n","        h = DenseLayer(L_in, D, A = lambda x : x)\n","        self.decoder_layers.append(h)\n","\n","        current_layer = self.Z\n","        for layer in self.decoder_layers:\n","            current_layer = layer.forward(current_layer)\n","        \n","        logits = current_layer\n","        posterior_predictive_logits = logits\n","\n","        self.X_hat_distribution = Bernoulli(logits = logits)\n","\n","        self.posterior_predictive = self.X_hat_distribution.sample()\n","        self.posterior_predictive_probs = tf.nn.sigmoid(logits)\n","\n","        #... Prior Predictive ...#\n","        standard_normal = Normal(\n","            loc = np.zeros(L, dtype = np.float32),\n","            scale = np.ones(L, dtype = np.float32)\n","        )\n","\n","        Z_std = standard_normal.sample(1)\n","        current_layer = Z_std\n","        for layer in self.decoder_layers:\n","            current_layer = layer.forward(current_layer)\n","        logits = current_layer\n","\n","        prior_predictive_dist = Bernoulli(logits = logits)\n","        self.prior_predictive = prior_predictive_dist.sample()\n","        self.prior_predictive_probs = tf.nn.sigmoid(logits)\n","\n","        self.Z_input = tf.placeholder(tf.float32, shape = (None, L))\n","        current_layer = self.Z_input\n","        for layer in self.decoder_layers:\n","            current_layer = layer.forward(current_layer)\n","        logits = current_layer\n","        self.prior_predictive_from_input_probs = tf.nn.sigmoid(logits)\n","\n","        #... Cost ...#\n","        if st is None:\n","                kl = -1 * tf.log(self.stddev) + 0.5*(self.stddev**2 + self.means**2) - 0.5\n","                kl = tf.reduce_sum(kl, axis=1)\n","        else:\n","            kl = tf.reduce_sum(\n","                tf.contrib.distributions.kl_divergence(\n","                self.Z.distribution, standard_normal\n","                ),\n","                1  \n","            )\n","          \n","        expected_log_likelihood = tf.reduce_sum(\n","            self.X_hat_distribution.log_prob(self.X),\n","            1\n","            )\n","\n","        self.ELBO =  tf.reduce_sum(expected_log_likelihood - kl)\n","        self.train_op = tf.train.RMSPropOptimizer(learning_rate = 0.001).minimize(-self.ELBO)\n","\n","        self.init_op = tf.global_variables_initializer()\n","        self.sess = tf.InteractiveSession()\n","        self.sess.run(self.init_op)\n","    \n","\n","    def get_mnist(self):\n","        \"\"\" \n","        To obtain the Mnist dataset in the required format, simply execute the get_mnist() function provided\n","        in the class itself. For other datasets, please ensure that the dataset is arranged in \n","        (Number_of_examples, features) format. \n","        \"\"\"\n","        (X, Y), (_,_) = mnist.load_data()\n","        X = X.reshape(60000, 784)\n","        X = X/ 255.0 # data is from 0..255\n","        return X, Y\n","\n","    \n","\n","    def fit(self, X, epochs = 30, batch_size = 64):\n","        \"\"\"\n","        Fit the data by training the model on it for a specified number of epochs and batch_size. \n","        It is best if the data is normalised befor it is fed in for training. \n","        \"\"\"\n","        costs = []\n","        n_batches = len(X) // batch_size\n","        print(\"Number of Batches:\" , n_batches)\n","\n","        for i in range(epochs):\n","            print(\"Epoch \", i, \": \" )\n","            np.random.shuffle(X)\n","            \n","            for j in range(n_batches):\n","                batch = X[j * batch_size : (j+1) * batch_size]\n","                _, c, = self.sess.run((self.train_op, self.ELBO), feed_dict = {self.X : batch})\n","                c = c/ batch_size\n","                costs.append(c)\n","                if j%100 == 0:\n","                    print(\"Iter: %d, Cost: %0.3f\" % (j, c))\n","        plt.plot(costs)\n","        plt.show()\n","\n","\n","    def transform(self, X):\n","        \"\"\"\n","        Calculate the mean obtained as the final layer of the encoder.\n","        \"\"\"\n","        return self.sess.run(\n","            self.means,\n","            feed_dict = {self.X: X}\n","        )\n","\n","\n","    def prior_predictive_with_input(self, Z):\n","        \"\"\"\n","        Calculate the prior probability for a given distributuion Z\n","        \"\"\"\n","        return self.sess.run(\n","            self.prior_predictive_from_input_probs, \n","            feed_dict = {self.Z: Z}\n","        )\n","\n","\n","    def posterior_predicitve_sample(self, X):\n","        \"\"\"\n","        Obtain posterior predictive sample for given X\n","        \"\"\"\n","        return self.sess.run(self.posterior_predictive, feed_dict = {self.X : X})\n","\n","    \n","    def prior_predictive_sample_with_probs(self):\n","        \"\"\"\n","        Obtain prior predictive sample with standard normal as the prior\n","        \"\"\"\n","        return self.sess.run((self.prior_predictive, self.prior_predictive_probs))\n","\n","\n","\n","\n","#... Executes first when file runs ...#\n","if __name__ == '__main__':\n","    # Create Vae class object\n","    vae = Variational_Autoencoder(784, [200, 100])\n","\n","    # Get data\n","    X, Y = vae.get_mnist()\n","\n","    # Convert data to binary\n","    X = (X > 0.5).astype(np.float32)\n","\n","    # Fit data to model\n","    vae.fit(X)\n","\n","    # Display samples and original images\n","    done = False\n","    while not done:\n","        i = np.random.choice(len(X))\n","        x = X[i]\n","        im = vae.posterior_predicitve_sample([x]).reshape(28, 28)\n","        plt.subplot(1, 2, 1)\n","        plt.imshow(x.reshape(28, 28), cmap = 'gray')\n","        plt.title(\"Original\")\n","\n","        plt.subplot(1, 2, 2)\n","        plt.imshow(im, cmap = 'gray')\n","        plt.title(\"Sample\")\n","\n","        plt.show()\n","\n","        ans = input(\"Generate Another\")\n","        if ans and ans[0] in ('n' or 'N'):\n","            done  = True\n","\n","    #Display Prior samples and probs\n","    done = False\n","    while not done:\n","        im, probs = vae.prior_predictive_sample_with_probs()\n","        im = im.reshape(28, 28)\n","        probs = probs.reshape(28, 28)\n","        plt.subplot(1, 2, 1)\n","        plt.imshow(im, cmap = 'gray')\n","        plt.title(\"Prior Predictive Sample\")\n","\n","        plt.subplot(1, 2, 2)\n","        plt.imshow(probs, cmap = 'gray')\n","        plt.title(\"Prior Predicitve probs\")\n","\n","        plt.show()\n","\n","        ans = input(\"Generate Another\")\n","        if ans and ans[0] in ('n' or 'N'):\n","            done  = True"],"execution_count":0,"outputs":[]}]}