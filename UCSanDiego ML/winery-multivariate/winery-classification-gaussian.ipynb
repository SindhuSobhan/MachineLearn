{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winery classification with the multivariate Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we return to winery classification, using the full set of 13 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load in the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we start by loading in the Wine data set. Make sure the file `wine.data.txt` is in the same directory as this notebook.\n",
    "\n",
    "Recall that there are 178 data points, each with 13 features and a label (1,2,3). As before, we will divide this into a training set of 130 points and a test set of 48 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard includes\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Useful module for dealing with the Gaussian density\n",
    "from scipy.stats import norm, multivariate_normal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set.\n",
    "data = np.loadtxt('wine.data.txt', delimiter=',')\n",
    "# Names of features\n",
    "featurenames = ['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash','Magnesium', 'Total phenols', \n",
    "                'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', \n",
    "                'OD280/OD315 of diluted wines', 'Proline']\n",
    "# Split 178 instances into training set (trainx, trainy) of size 130 and test set (testx, testy) of size 48\n",
    "np.random.seed(0)\n",
    "perm = np.random.permutation(178)\n",
    "trainx = data[perm[0:130],1:14]\n",
    "trainy = data[perm[0:130],0]\n",
    "testx = data[perm[130:178], 1:14]\n",
    "testy = data[perm[130:178],0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fit a Gaussian generative model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function that fits a Gaussian generative model to the data.\n",
    "For each class (`j=1,2,3`), we have:\n",
    "* `pi[j]`: the class weight\n",
    "* `mu[j,:]`: the mean, a 13-dimensional vector\n",
    "* `sigma[j,:,:]`: the 13x13 covariance matrix\n",
    "\n",
    "This means that `pi` is a 4x1 array (Python arrays are indexed starting at zero, and we aren't using `j=0`), `mu` is a 4x13 array and `sigma` is a 4x13x13 array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_generative_model(x,y, features):\n",
    "    k = len(np.unique(y))  # labels 1,2,...,k\n",
    "    d = len(features)  # number of features\n",
    "    mu = np.zeros((k+1,d))\n",
    "    sigma = np.zeros((k+1,d,d))\n",
    "    pi = np.zeros(k+1)\n",
    "    for label in range(1,k+1):\n",
    "        indices = (y == label)\n",
    "        mu[label] = np.mean(x[np.ix_(indices, features)], axis = 0)\n",
    "        sigma[label] = np.cov(x[np.ix_(indices, features)], rowvar=0, bias=1)\n",
    "        pi[label] = float(sum(indices))/float(len(y))\n",
    "    return mu, sigma, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.        ]\n",
      " [13.78534884  2.42790698  2.99627907]\n",
      " [12.31092593  2.22703704  2.10907407]\n",
      " [13.15969697  2.40090909  0.75727273]]\n",
      "[[[ 0.          0.          0.        ]\n",
      "  [ 0.          0.          0.        ]\n",
      "  [ 0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.23325279 -0.00393532  0.07526874]\n",
      "  [-0.00393532  0.03677469 -0.00140779]\n",
      "  [ 0.07526874 -0.00140779  0.15240941]]\n",
      "\n",
      " [[ 0.2819047  -0.03746578 -0.03911211]\n",
      "  [-0.03746578  0.11230233  0.09116207]\n",
      "  [-0.03911211  0.09116207  0.56869729]]\n",
      "\n",
      " [[ 0.2851787   0.01545785  0.01006887]\n",
      "  [ 0.01545785  0.02675978  0.02115399]\n",
      "  [ 0.01006887  0.02115399  0.07375317]]]\n",
      "[0.         0.33076923 0.41538462 0.25384615]\n"
     ]
    }
   ],
   "source": [
    "# Fit a Gaussian generative model to the training data\n",
    "mu, sigma, pi = fit_generative_model(trainx, trainy, [0,2,6])\n",
    "print(mu, sigma, pi, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"magenta\">**For you to do**</font>: Define a general purpose testing routine that takes as input:\n",
    "* the arrays `pi`, `mu`, `sigma` defining the generative model, as above\n",
    "* the test set (points `tx` and labels `ty`)\n",
    "* a list of features `features` (chosen from 0-12)\n",
    "\n",
    "It should return the number of mistakes made by the generative model on the test data, *when restricted to the specified features*. For instance, using the just three features 2 (`'Ash'`), 4 (`'Magnesium'`) and 6 (`'Flavanoids'`) results in 7 mistakes (out of 48 test points), so \n",
    "\n",
    "        `test_model(mu, sigma, pi, [2,4,6], testx, testy)` \n",
    "\n",
    "should print 7/48.\n",
    "\n",
    "**Hint:** The way you restrict attention to a subset of features is by choosing the corresponding coordinates of the full 13-dimensional mean and the appropriate submatrix of the full 13x13 covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now test the performance of a predictor based on a subset of features\n",
    "def test_model(mu, sigma, pi, features, tx, ty):\n",
    "    if len(features) > 1:\n",
    "        if features[0] == features[1]: # need f1 != f2\n",
    "            print(\"Please choose different features for f1 and f2.\")\n",
    "            return  \n",
    "        \n",
    "    mu, sigma, pi = fit_generative_model(tx, ty, features)\n",
    "    \n",
    "    k = len(np.unique(ty))\n",
    "    nt = len(ty) # Number of test points\n",
    "    score = np.zeros((nt,k+1))\n",
    "    for i in range(0,nt):\n",
    "        for label in range(1,k+1):\n",
    "            score[i,label] = np.log(pi[label]) + \\\n",
    "            multivariate_normal.logpdf(testx[i,features], mean=mu[label,:], cov=sigma[label,:,:])\n",
    "    predictions = np.argmax(score[:,1: k+1], axis=1) + 1\n",
    "    # Finally, tally up score\n",
    "    errors = np.sum(predictions != testy)\n",
    "    print(\"Test error using feature(s): \")\n",
    "    for f in features:\n",
    "        print(\"'\" + featurenames[f] + \"'\" + \" \")\n",
    "    print(\"Errors: \" + str(errors) + \"/\" + str(nt))# Now test the performance of a predictor based on a subset of features\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Fast exercises</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note down the answers to these questions. You will need to enter them as part of this week's assignment.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1. How many errors are made on the test set when using the single feature 'Ash'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error using feature(s): \n",
      "'Ash' \n",
      "Errors: 27/48\n"
     ]
    }
   ],
   "source": [
    "errors = test_model(mu, sigma, pi, [2], testx, testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2. How many errors when using 'Alcohol' and 'Ash'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error using feature(s): \n",
      "'Alcohol' \n",
      "'Malic acid' \n",
      "Errors: 8/48\n",
      "Test error using feature(s): \n",
      "'Alcohol' \n",
      "'Ash' \n",
      "Errors: 11/48\n",
      "Test error using feature(s): \n",
      "'Malic acid' \n",
      "'Ash' \n",
      "Errors: 16/48\n",
      "\n",
      "\n",
      "\n",
      "Min. error for\n",
      "Alcohol and Malic acid\n",
      "\n",
      "Incorrect classifications = 8.0\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "k = len(np.unique(trainy))\n",
    "errors = 1e6 * np.ones((k+1,1))\n",
    "min_err = {'val':[1e4], 'arg':[]}\n",
    "    \n",
    "for i in range(k):\n",
    "    for j in range(i, k):\n",
    "        if i != j:\n",
    "            errors[i + j] = test_model(mu, sigma, pi, [i, j], testx, testy)\n",
    "        \n",
    "            if np.min(errors) < min_err['val']:\n",
    "                min_err['val'] = np.min(errors)\n",
    "                min_err['arg'].append([i, j])\n",
    "                \n",
    "print('\\n\\n\\nMin. error for'+ '\\n' + featurenames[min_err['arg'][0][0]] + ' and ' + featurenames[min_err['arg'][0][1]]) \n",
    "print('\\n' + 'Incorrect classifications = ' + str(min_err['val']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3. How many errors when using 'Alcohol', 'Ash', and 'Flavanoids'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error using feature(s): \n",
      "'Alcohol' \n",
      "'Ash' \n",
      "'Flavanoids' \n",
      "Errors: 2/48\n"
     ]
    }
   ],
   "source": [
    "test_model(mu, sigma, pi, [0,2,6], testx, testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4. How many errors when using all 13 features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error using feature(s): \n",
      "'Alcohol' \n",
      "'Malic acid' \n",
      "'Ash' \n",
      "'Alcalinity of ash' \n",
      "'Magnesium' \n",
      "'Total phenols' \n",
      "'Flavanoids' \n",
      "'Nonflavanoid phenols' \n",
      "'Proanthocyanins' \n",
      "'Color intensity' \n",
      "'Hue' \n",
      "'OD280/OD315 of diluted wines' \n",
      "'Proline' \n",
      "Errors: 0/48\n"
     ]
    }
   ],
   "source": [
    "test_model(mu, sigma, pi, range(0,13), testx, testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard includes\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Useful module for dealing with the Gaussian density\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "\n",
    "def load_and_process_data(filename):\n",
    "    # Load data set.\n",
    "    data = np.loadtxt(filename, delimiter=',')\n",
    "    # Names of features\n",
    "    featurenames = ['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash','Magnesium', 'Total phenols', \n",
    "                'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', \n",
    "                'OD280/OD315 of diluted wines', 'Proline']\n",
    "    # Split 178 instances into training set (trainx, trainy) of size 130 and test set (testx, testy) of size 48\n",
    "    np.random.seed(0)\n",
    "    perm = np.random.permutation(178)\n",
    "    trainx = data[perm[0:130],1:14]\n",
    "    trainy = data[perm[0:130],0]\n",
    "    testx = data[perm[130:178], 1:14]\n",
    "    testy = data[perm[130:178],0]\n",
    "    \n",
    "    return trainx, trainy, testx, testy, featurenames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fit_naive_generative_model(trainx, trainy, features):\n",
    "    n_class = len(np.unique(trainy))\n",
    "    n_feat = len(features)\n",
    "    \n",
    "    mu = np.zeros((n_class, n_feat))\n",
    "    var = np.zeros((n_class, n_feat))\n",
    "    pi = np.zeros((n_class, 1))\n",
    "    \n",
    "    for cl in range(n_class):\n",
    "        for feat in range(n_feat):\n",
    "            feature = features[feat]\n",
    "            indices = (trainy == cl+1)\n",
    "            mu[cl, feat] = np.mean(trainx[indices, feature])\n",
    "            var[cl, feat] = np.var(trainx[indices, feature])\n",
    "            \n",
    "    pi = np.array([np.sum(trainy == 1), np.sum(trainy == 2), np.sum(trainy == 3)]) / float(len(trainy))\n",
    "    \n",
    "    return mu, var, pi \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fit_normal(x, mu, var):\n",
    "    P = norm.logpdf(x, mu, np.sqrt(var))\n",
    "    return P\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "def predict_class(tx, ty, features, mu, var, pi):\n",
    "    n_feat = len(features)\n",
    "    n_class = len(np.unique(ty))\n",
    "    n = tx.shape[0]\n",
    "    \n",
    "    P = np.zeros((n, n_class, n_feat))\n",
    "    Scores = np.zeros((n, n_class))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for cl in range(0, n_class):\n",
    "            for feat in range(n_feat):\n",
    "                feature = features[feat]\n",
    "                P[i, cl, feat] = fit_normal(tx[i, feature], mu[cl, feat], var[cl, feat])\n",
    "            Scores[i, cl] = np.log(pi[cl]) +  np.sum(P[i, cl, :])\n",
    "        \n",
    "    predictions = np.argmax(Scores, axis=1) + 1\n",
    "    \n",
    "    return predictions, P, Scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def accuracy(predictions, ty, features, featurenames):\n",
    "    errors = np.sum(predictions != ty)\n",
    "    \n",
    "    print(\"Test error using feature(s): \")\n",
    "    \n",
    "    for f in features:\n",
    "        print(\"'\" + featurenames[f] + \"'\" + \" \")\n",
    "        \n",
    "    print(\"Errors: \" + str(errors) + \"/\" + str(len(ty)) + \"= \" + str(100* ((len(ty) - errors) / len(ty))) + \"%\")\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def NaiveBayes(file, features, output = False):\n",
    "    \"\"\"Function to obtain the Naive Bayes Classifier for a dataset contained in a comma separated text file.\n",
    "    \n",
    "    Inputs: \n",
    "    1. Filename in .txt format (a string)\n",
    "    2. Features or number of the columns that contain the desired features\n",
    "    3. Output if true, will display Errors values, Probabilitites and Scores for all data points, classes and desired features\n",
    "    \n",
    "    Output:\n",
    "    If output is True:\n",
    "    Error, Probabilities for each class and each feature of each prediction, and Scores for each class for each prediction\n",
    "    \n",
    "    Default: (also if output is False)\n",
    "    Featurenames and Associated Accuracy\"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    n_out = 3\n",
    "    n_in = 3\n",
    "    \n",
    "    try:\n",
    "        if (len(np.unique(features)) != len(features)) or (np.sign(np.prod(features)) == -1):\n",
    "            print(\"Please Enter Unique positive numbers for features\")\n",
    "            return [None]*n_out\n",
    "        else:\n",
    "            #Load and Process Data\n",
    "            trainx, trainy, testx, testy, featurenames = load_and_process_data(file)\n",
    "            \n",
    "            #Fit generative model\n",
    "            mu, var, pi = fit_naive_generative_model(trainx, trainy, features)\n",
    "    \n",
    "            #Find predictions for test cases\n",
    "            predictions, P, Scores = predict_class(testx, testy, features, mu, var, pi)\n",
    "    \n",
    "            #Find Accuracy\n",
    "            errors = accuracy(predictions, testy, features, featurenames)\n",
    "            \n",
    "            if output:\n",
    "                return errors, P, Scores\n",
    "    \n",
    "    except TypeError:\n",
    "        print(\"Please Enter the feature set as a list of numbers or an 1D array of numbers or check the type of data entered. It must be numerical\")\n",
    "        return [None]*n_out\n",
    "    \n",
    "    except IndexError:\n",
    "        print(\"Please Enter a valid Index between 0 and the number of features in the dataset\")\n",
    "        return [None]*n_out\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error using feature(s): \n",
      "'Alcohol' \n",
      "'Malic acid' \n",
      "'Ash' \n",
      "'Alcalinity of ash' \n",
      "'Magnesium' \n",
      "'Total phenols' \n",
      "'Flavanoids' \n",
      "'Nonflavanoid phenols' \n",
      "'Proanthocyanins' \n",
      "'Color intensity' \n",
      "'Hue' \n",
      "'OD280/OD315 of diluted wines' \n",
      "Errors: 2/48= 95.83333333333334%\n"
     ]
    }
   ],
   "source": [
    "NaiveBayes('wine.data.txt', range(12))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
